{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7e1c911ed01e39ab2a0c9a367fc26088080f4122"
   },
   "outputs": [],
   "source": [
    "bert_path = 'chinese_L-12_H-768_A-12'\n",
    "VOCAB = os.path.join(bert_path, 'vocab.txt')\n",
    "MODEL = os.path.join(bert_path, 'bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "2380f086d27f96acb54cd0f05cfb92b4f75bf5da"
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch_pretrained_bert==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b60abf89faaf45cc5965f4caecab62c3cdbc62b"
   },
   "source": [
    "# Just list which models can used\n",
    "\n",
    "            'bert-base-uncased'\n",
    "            'bert-large-uncased'\n",
    "            'bert-base-cased': \n",
    "            'bert-large-cased'\n",
    "            'bert-base-multilingual-uncased'\n",
    "            'bert-base-multilingual-cased'\n",
    "            'bert-base-chinese'\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7bc99e8cf9434bc63e4941cce3aec071dc83105e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6193344fe927573b3f67da350021854d7e282ea9"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"input\"):\n",
    "    TRAIN_CSV_PATH = 'input/train.csv'\n",
    "    TEST_CSV_PATH = 'input/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "600edbf529304d553c9db781e5e6bd1bbcf17108"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhome/cayonliow/anaconda3/envs/sdml/lib/python3.7/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh                  title2_zh      label\n",
       "id                                                                         \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_CSV_PATH, index_col='id')\n",
    "test = pd.read_csv(TEST_CSV_PATH, index_col='id')\n",
    "cols = ['title1_zh', \n",
    "        'title2_zh', \n",
    "        'label']\n",
    "train = train.loc[:, cols]\n",
    "test = test.loc[:, cols]\n",
    "train.fillna('UNKNOWN', inplace=True)\n",
    "test.fillna('UNKNOWN', inplace=True)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "88b424495340472408a30827805034670e47ec04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'unrelated': 219313, 'agreed': 92973, 'disagreed': 8266})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9658fafebdb8f5b8b95221fec7ebb444b2ec978c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/run_classifier.py\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "04b5fcaff32c0e8bf13c586b6c4462a6cb00d7bc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection \\\n",
    "    import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "train, val= \\\n",
    "    train_test_split(\n",
    "        train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "0a86c71b6d2375d5e36be524dcffe634b3f6d3be"
   },
   "outputs": [],
   "source": [
    "label_list = ['unrelated', 'agreed', 'disagreed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "6193fe7713ff9a71cb8153a16e76819c18c3cf03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from run_classifier import *\n",
    "\n",
    "train_examples = [InputExample('train', row.title1_zh, row.title2_zh, row.label) for row in train.itertuples()]\n",
    "val_examples = [InputExample('val', row.title1_zh, row.title2_zh, row.label) for row in val.itertuples()]\n",
    "test_examples = [InputExample('test', row.title1_zh, row.title2_zh, 'unrelated') for row in test.itertuples()]\n",
    "\n",
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "75aa53708b72e3992990f06862a98c79299a325a"
   },
   "outputs": [],
   "source": [
    "orginal_total = len(train_examples)\n",
    "train_examples = train_examples[:int(orginal_total*0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ab0b13dd30fe5dd3da025d961e288bd4431075d5"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 64\n",
    "train_batch_size = train_batch_size // gradient_accumulation_steps\n",
    "output_dir = 'output'\n",
    "bert_model = 'bert-base-chinese'\n",
    "num_train_epochs = 3\n",
    "num_train_optimization_steps = int(\n",
    "            len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "cache_dir = \"model\"\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "max_seq_length = 128\n",
    "label_list = ['unrelated', 'agreed', 'disagreed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chinese_L-12_H-768_A-12/bert_model'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "28d44f3b50b9cef652ec7ffc2822df82b75293df",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2020 10:08:50 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file chinese_L-12_H-768_A-12/vocab.txt\n",
      "05/06/2020 10:08:51 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at model/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "05/06/2020 10:08:51 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file model/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpa7l1ng10\n",
      "05/06/2020 10:08:54 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "05/06/2020 10:08:56 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "05/06/2020 10:08:56 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(21128, 768)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): BertLayerNorm()\n",
       "       (dropout): Dropout(p=0.1)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (1): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (2): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (3): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (4): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (5): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (6): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (7): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (8): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (9): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (10): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "         (11): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): BertLayerNorm()\n",
       "               (dropout): Dropout(p=0.1)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): BertLayerNorm()\n",
       "             (dropout): Dropout(p=0.1)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1)\n",
       "   (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       " ), <pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7fa86f7afc10>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(VOCAB)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese',\n",
    "              cache_dir=cache_dir,\n",
    "              num_labels = 3)\n",
    "model.to(device)\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "b93a5f7d8ae1012a7e606e9d1581722a4a2e329e"
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=learning_rate,\n",
    "                             warmup=warmup_proportion,\n",
    "                             t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "b098b3962887d0bede5052b4cf25902d9fc5c4fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2020 10:08:58 - INFO - run_classifier -   *** Example ***\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   guid: train\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   tokens: [CLS] 营 养 师 ： 补 充 这 4 种 营 养 能 帮 你 降 血 压 ， 你 一 样 都 不 吃 么 ？ [SEP] 刘 涛 担 心 前 夫 离 婚 后 找 不 到 另 一 半 ， 居 然 还 主 动 给 他 介 绍 女 人 [SEP]\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_ids: 101 5852 1075 2360 8038 6133 1041 6821 125 4905 5852 1075 5543 2376 872 7360 6117 1327 8024 872 671 3416 6963 679 1391 720 8043 102 1155 3875 2857 2552 1184 1923 4895 2042 1400 2823 679 1168 1369 671 1288 8024 2233 4197 6820 712 1220 5314 800 792 5305 1957 782 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   *** Example ***\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   guid: train\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   tokens: [CLS] 刘 涛 现 场 痛 哭 发 飙 要 离 婚 ， 直 接 把 旁 边 的 沈 腾 吓 蒙 了 [SEP] 她 长 相 酷 似 刘 涛 ， 是 陈 思 诚 前 女 友 ， 离 婚 后 的 她 活 出 了 真 正 的 自 己 ！ [SEP]\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_ids: 101 1155 3875 4385 1767 4578 1526 1355 7604 6206 4895 2042 8024 4684 2970 2828 3178 6804 4638 3755 5596 1405 5885 749 102 1961 7270 4685 6999 849 1155 3875 8024 3221 7357 2590 6411 1184 1957 1351 8024 4895 2042 1400 4638 1961 3833 1139 749 4696 3633 4638 5632 2346 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   *** Example ***\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   guid: train\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   tokens: [CLS] nba 被 兜 售 球 星 诞 生 ！ 1 亿 美 金 篮 板 王 ！ 火 箭 骑 士 同 抢 交 易 ？ [SEP] 猛 龙 总 裁 辟 谣 球 队 重 建 传 闻 ， 主 帅 的 去 留 需 综 合 多 方 面 考 虑 [SEP]\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_ids: 101 8391 6158 1055 1545 4413 3215 6414 4495 8013 122 783 5401 7032 5074 3352 4374 8013 4125 5055 7744 1894 1398 2843 769 3211 8043 102 4338 7987 2600 6161 6792 6469 4413 7339 7028 2456 837 7319 8024 712 2358 4638 1343 4522 7444 5341 1394 1914 3175 7481 5440 5991 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   *** Example ***\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   guid: train\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   tokens: [CLS] 海 口 飞 机 撒 药 治 白 蛾 [SEP] [UNK] 配 方 [UNK] [UNK] 味 道 [UNK] 没 变 ！ 飞 机 撒 药 治 白 蛾 谣 言 [SEP]\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_ids: 101 3862 1366 7607 3322 3054 5790 3780 4635 6042 102 100 6981 3175 100 100 1456 6887 100 3766 1359 8013 7607 3322 3054 5790 3780 4635 6042 6469 6241 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   *** Example ***\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   guid: train\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   tokens: [CLS] 网 曝 杜 海 涛 与 沈 梦 辰 已 分 手 疑 似 女 方 劈 腿 [SEP] 爆 ： 杜 海 涛 与 沈 梦 辰 已 分 手 疑 似 女 方 出 轨 [SEP]\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_ids: 101 5381 3284 3336 3862 3875 680 3755 3457 6801 2347 1146 2797 4542 849 1957 3175 1207 5597 102 4255 8038 3336 3862 3875 680 3755 3457 6801 2347 1146 2797 4542 849 1957 3175 1139 6758 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/06/2020 10:08:58 - INFO - run_classifier -   label: agreed (id = 1)\n",
      "05/06/2020 10:09:15 - INFO - run_classifier -   ***** Running training *****\n",
      "05/06/2020 10:09:15 - INFO - run_classifier -     Num examples = 57699\n",
      "05/06/2020 10:09:15 - INFO - run_classifier -     Batch size = 4\n",
      "05/06/2020 10:09:15 - INFO - run_classifier -     Num steps = 43272\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/14424)\n",
      "Fininshed: 10.00% (1442/14424)\n",
      "Fininshed: 19.99% (2884/14424)\n",
      "Fininshed: 29.99% (4326/14424)\n",
      "Fininshed: 39.99% (5768/14424)\n",
      "Fininshed: 49.99% (7210/14424)\n",
      "Fininshed: 59.98% (8652/14424)\n",
      "Fininshed: 69.98% (10094/14424)\n",
      "Fininshed: 79.98% (11536/14424)\n",
      "Fininshed: 89.98% (12978/14424)\n",
      "Fininshed: 99.97% (14420/14424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 1/3 [1:27:44<2:55:29, 5264.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/14424)\n",
      "Fininshed: 10.00% (1442/14424)\n",
      "Fininshed: 19.99% (2884/14424)\n",
      "Fininshed: 29.99% (4326/14424)\n",
      "Fininshed: 39.99% (5768/14424)\n",
      "Fininshed: 49.99% (7210/14424)\n",
      "Fininshed: 59.98% (8652/14424)\n",
      "Fininshed: 69.98% (10094/14424)\n",
      "Fininshed: 79.98% (11536/14424)\n",
      "Fininshed: 89.98% (12978/14424)\n",
      "Fininshed: 99.97% (14420/14424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 2/3 [2:55:13<1:27:40, 5260.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/14424)\n",
      "Fininshed: 10.00% (1442/14424)\n",
      "Fininshed: 19.99% (2884/14424)\n",
      "Fininshed: 29.99% (4326/14424)\n",
      "Fininshed: 39.99% (5768/14424)\n",
      "Fininshed: 49.99% (7210/14424)\n",
      "Fininshed: 59.98% (8652/14424)\n",
      "Fininshed: 69.98% (10094/14424)\n",
      "Fininshed: 79.98% (11536/14424)\n",
      "Fininshed: 89.98% (12978/14424)\n",
      "Fininshed: 99.97% (14420/14424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [4:26:04<00:00, 5321.57s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "\n",
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, max_seq_length, tokenizer)\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "model.train()\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    total_step = len(train_data) // train_batch_size\n",
    "    ten_percent_step = total_step // 10\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "#         print('step')\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "        if step % ten_percent_step == 0:\n",
    "            print(\"Fininshed: {:.2f}% ({}/{})\".format(step/total_step*100, step, total_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "bb25f1d8d6226c8d4bfe22ed1efcee1be368e7ff"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WEIGHTS_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-089e4a27b51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save a trained model and the associated configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'module'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# Only save the model it-self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput_config_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WEIGHTS_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save a trained model and the associated configuration\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "with open(output_config_file, 'w') as f:\n",
    "    f.write(model_to_save.config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa7c59dd0eacdd89c629e97d91be718f70f7c5dc"
   },
   "outputs": [],
   "source": [
    "# Load a trained model and config that you have fine-tuned\n",
    "config = BertConfig(output_config_file)\n",
    "model = BertForSequenceClassification(config, num_labels=len(label_list))\n",
    "model.load_state_dict(torch.load(output_model_file))\n",
    "model.to(device)  # important to specific device\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35c1906644d8c1d1789bc105884142e967bcfd87"
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fcfd222826070652e34c494502f1b7f3288d180"
   },
   "outputs": [],
   "source": [
    "# val\n",
    "eval_examples = val_examples\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, max_seq_length, tokenizer)\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss/nb_tr_steps\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'global_step': global_step,\n",
    "          'loss': loss}\n",
    "\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b886af345aefc2e940e57a400f68103863b4f9b"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5fe27781f29d013f4409cd3ceea1a0058786ebd"
   },
   "outputs": [],
   "source": [
    "!mkdir output\n",
    "!ls output\n",
    "! cat output/eval_results.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ca117e30ef9c3acd4efc7a89b73bbf44e173ff0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "741b42a6de4d76a48001165d0efdb59a22f5e420"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, examples, label_list, eval_batch_size=128):\n",
    "    model.to(device)\n",
    "    eval_examples = examples\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    \n",
    "    res = []\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "#         label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "#         print(logits)\n",
    "        res.extend(logits.argmax(-1))\n",
    "#         label_ids = label_ids.to('cpu').numpy()\n",
    "#         tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "#         eval_loss += tmp_eval_loss.mean().item()\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#         nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "#     eval_loss = eval_loss / nb_eval_steps\n",
    "#     eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "#     loss = tr_loss/nb_tr_steps \n",
    "#     result = {'eval_loss': eval_loss,\n",
    "#               'eval_accuracy': eval_accuracy,\n",
    "#               'global_step': global_step,\n",
    "#               'loss': loss}\n",
    "\n",
    "#     output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "#     with open(output_eval_file, \"w\") as writer:\n",
    "#         logger.info(\"***** Eval results *****\")\n",
    "#         for key in sorted(result.keys()):\n",
    "#             logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#             writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e20b72f980d3a0112a1b6ac1c67d0f1823c1f4d9"
   },
   "outputs": [],
   "source": [
    "res = predict(model, tokenizer, test_examples, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1523e50eb6a113b13a4de1f3926b73e62ba36e97"
   },
   "outputs": [],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf78ac2d2be4dbb4b63f69c5e82c34351df6c8df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bab40a86d6cd07aa1456f37fbdc0d26f94000309"
   },
   "outputs": [],
   "source": [
    "predict(model, tokenizer, test_examples[:10], label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79c0163e94d9c5f6b1e5ae4a71ebf65f4ff58feb"
   },
   "outputs": [],
   "source": [
    "cat_map = {idx:lab for idx, lab in enumerate(label_list)}\n",
    "res = [cat_map[c] for c  in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dcc715e51af5120090589a9b91169bbf8d01d705"
   },
   "outputs": [],
   "source": [
    "#　For Submission\n",
    "\n",
    "test['Category'] = res\n",
    "\n",
    "\n",
    "submission = test \\\n",
    "    .loc[:, ['Category']] \\\n",
    "    .reset_index()\n",
    "\n",
    "submission.columns = ['Id', 'Category']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
